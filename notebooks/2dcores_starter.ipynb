{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Starter for Cores"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In this notebook we provide an explanation of the different choices for Cores. For each of them we include: \n",
    "- a written description of the Core\n",
    "- a code demo how to use the Core"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### The model has two main parts"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The baseline CNN model (which is mostly based on [this work](https://openreview.net/forum?id=Tp7kI90Htd)) is constructed from two main parts:\n",
    "- **core**: the core aims to (nonlinearly) extract features that are common between neurons. That is, we assume there exist a set of features that all neurons use but combine them in their own unique way.\n",
    "- **readout**: once the core extracts the feautures, then a neuron reads out from those features by simply linearly combining those features into a single value. Finally, by passing this single value through a final nonlinarity (in this case `ELU() + 1`) we make sure that the model output is positive and we get the inferred firing rate of the neuron."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Currently (March 2024) there are 4 versions of Core modules"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "1. Stacked2dCore\n",
    "2. RotationEquivariant2dCore\n",
    "3. TransferLearningCore\n",
    "4. SE2dCore\n",
    "\n",
    "Descriptions of each **Core** module can be found in the neuralpredictors/layers/cores/conv2d.py. However, for convenience we will include descriptions in each dedicated block. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before learning how to use the cores, let's load the data. Refer to notebook **0_baseline_CNN.ipynb** for more explanations."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from nnfabrik.builder import get_data\n",
    "\n",
    "device = \"cpu\"\n",
    "random_seed = 42\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Instantiate DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# loading the SENSORIUM+ dataset\n",
    "filenames = ['/project/data/static22846-10-16-GrayImageNet-94c6ff995dac583098847cfecd43e7b6.zip', ]\n",
    "\n",
    "dataset_fn = 'sensorium.datasets.static_loaders'\n",
    "dataset_config = {'paths': filenames,\n",
    "                 'normalize': True,\n",
    "                 'include_behavior': False,\n",
    "                 'include_eye_position': True,\n",
    "                 'batch_size': 32,\n",
    "                 'scale':1,\n",
    "                 'cuda': False\n",
    "                 }\n",
    "\n",
    "dataloaders = get_data(dataset_fn, dataset_config)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Access the input images in a batch of train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = dataloaders['train']['22846-10-16']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(dl))\n",
    "images = batch.images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1, 144, 256])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next code cell contains configuration parameters for the core. We will explore what they mean after introducing available choices for core. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = {\n",
    "    # core args\n",
    "    'input_channels': 1,\n",
    "    'input_kern': 9,\n",
    "    'hidden_kern': 7,\n",
    "    'hidden_channels': 64,\n",
    "    'layers': 4,\n",
    "    'depth_separable': True,\n",
    "    'stack': -1,\n",
    "    'gamma_input': 6.3831,\n",
    "    'pad_input': True\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Stacked2dCore"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Made up of layers layers of nn.sequential modules.\n",
    "Allows for the flexible implementations of many different architectures, such as convolutional layers,\n",
    "or self-attention layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 64, 136, 248])\n"
     ]
    }
   ],
   "source": [
    "from neuralpredictors.layers.cores import Stacked2dCore \n",
    "model_stacked2dcore = Stacked2dCore(**model_config)\n",
    "stacked2dcore_out = model_stacked2dcore(images)\n",
    "print(stacked2dcore_out.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RotationEquivariant2dCore "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A core built of 2d rotation-equivariant layers. For more info refer to https://openreview.net/forum?id=H1fU8iAqKX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralpredictors.layers.cores import RotationEquivariant2dCore\n",
    "model_rotationequivariant = RotationEquivariant2dCore(**model_config)\n",
    "rotationequvariant_out = model_rotationequivariant(images)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SE2dCore"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An extension of the Stacked2dCore class. The convolutional layers can be set to be either depth-separable\n",
    "(as used in the popular MobileNets) or based on self-attention (as used in Transformer networks).\n",
    "Additionally, a SqueezeAndExcitation layer (also called SE-block) can be added after each layer or the n final layers. Finally, it is also possible to make this core fully linear, by disabling all nonlinearities.\n",
    "This makes it effectively possible to turn a core+readout CNN into a LNP-model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralpredictors.layers.cores import SE2dCore\n",
    "model_se2dcore = SE2dCore(**model_config)\n",
    "se2dcore_out = model_se2dcore(images)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TransferLearningCore"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Core based on popular image recognition networks from torchvision such as VGG or AlexNet. Can be instantiated as random or pretrained. Core is frozen by default, which can be changed with the fine_tune argument."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try loading pretrained VGG16 and AlexNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralpredictors.layers.cores import TransferLearningCore\n",
    "model_transfer_learning = TransferLearningCore(tl_model_name='vgg16', **model_config)\n",
    "vgg16_core_out = model_transfer_learning(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralpredictors.layers.cores import TransferLearningCore\n",
    "model_transfer_learning = TransferLearningCore(tl_model_name = 'alexnet', **model_config)\n",
    "alexnet_core_out = model_transfer_learning(images)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have created instances of core models, such as **model_stacked2dcore, vgg16_core**. Now we explore how the main configuration parameters (five parameters to be exact) for the model influence our input images and the core output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = {\n",
    "    # core args\n",
    "    'input_channels': 1,\n",
    "    'input_kern': 9,\n",
    "    'hidden_kern': 7,\n",
    "    'hidden_channels': 64,\n",
    "    'layers': 3\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(`input_channels` = 1) specifes the number of input channels. In the dataset we have loaded we have gray scale images, so the number of input channels is 1. If you have colored RGB images, you will need to set the input channels to 3. If you want your model to take into account also the behavioral parameters, such as pupil size and running speed, you accordingly increase the input channels to 5. \n",
    "(`hidden_channels` = 64) specifies the number of hidden channels. This is freely up to the user to define. If you want to have different sized hidden layers, you can pass on a list of length (`layers`) \n",
    "\n",
    "(`input_kern` = 9) sets the size of convolutional kernel at the first layer. \n",
    "(`hidden_kern` = 7) sets the size of convolutional kernel for all hidde layers. If you want to have different sized convolutional kernels, you can pass on a list of length (`layers`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 192, 144, 256])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_stacked2dcore = Stacked2dCore(**model_config)\n",
    "stacked2dcore_out = model_stacked2dcore(images)\n",
    "stacked2dcore_out.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We see that the output of the core has:\n",
    "- **192** channels as specified by the `hidden_channels` * `layers`. This implies the core ouputs a stack of all hidden layers. If you want your model to output only the last hidden layer, then set the (`stack` = -1), which is another model configuration parameter. \n",
    "- a height of **144** and a width of **256** same as input images. This implies that the images are padded with zeros to keep the same input dimensions at the output. This is specified throught the (`pad_input` = True) model configuration parameter. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### References\n",
    "- Code for the [model function](https://github.com/sinzlab/sensorium/blob/8660c0c925b3944e723637db4725083f84ee28c3/sensorium/models/models.py#L17)\n",
    "- Code for the [core Module](https://github.com/sinzlab/neuralpredictors/blob/0d3d793cc0e1f55ec61c5f9f7a98318b5241a2e9/neuralpredictors/layers/cores/conv2d.py#L27)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
